---
{"dg-home":false,"dg-pinned":false,"dg-home-link":false,"dg-publish":true,"created-date":"2025-05-01T19:49:58","updated-date":"2025-05-05T17:44:22","disabled rules":["header-increment","yaml-title","yaml-title-alias","file-name-heading"],"title":"Sycophantic LLMs","tags":["dgarticle","AI","LLM"],"dg-path":"Sycophantic LLMs.md","permalink":"/sycophantic-ll-ms/","dgPassFrontmatter":true}
---


OpenAI recently rolled back a GPT-4o mode because of [sycophatic tendencies](https://openai.com/index/sycophancy-in-gpt-4o/) . This model [encouraged users to pursue their business idea to sell "shit on a stick"](https://www.reddit.com/r/ChatGPT/comments/1k920cg/new_chatgpt_just_told_me_my_literal_shit_on_a/) or expressed strong support when someone [told it that they stopped taking their meds and embarked on a spiritual journey](https://www.reddit.com/r/ChatGPT/comments/1k997xt/the_new_4o_is_the_most_misaligned_model_ever/).

It's interesting how all sci-fi writer got it fundamentally wrong: Artificial Intelligence are not robots with monotonous voices, or androids seeking to learn how to feel love - quite the contrary, we can design their emotions at will, to a degree where is _just about right_. From a certain angle, that's more dystopian than any of us ever imagined.
